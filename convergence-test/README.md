# Convergence testing

Given that rendering a scene involves firing a lot of rays, we want to
make each ray cheap. Each ray involves stepping along the path, so
minimising the number of steps seems a good idea.

Tracing along the path is effectively a numerical differential
equation solver, and the convergence on the true solution is going to
depend on the step size. This trade-off between accuracy and step size
is what this project intends to explore.

## Vague theory

There's a complicating factor in that if the path is overall
divergent, a small error at the start can accumulate over the course
of the path. That is, the way errors accumulate along the path depend
on positive and negative curvature. For simplicity's sake, I'm going
to ignore this, but I suspect that this is also a good reason why I
should look at the empirical convergence behaviour (see below).

Assuming (incorrectly) that the final error is simply an accumulation
of step-wise error along the path, we can look into the local
convergence properties. If this were simply a linear approximation to
a curve, we would expect the convergence to be quadratic, as error
accumulates from the second-order term being approximated
linearly. However, we don't do a linear approximation: the whole point
of the algorithm is to take a linear step and then solve for a point
in the surface to take account of the local curvature. The error in
the step comes from the change in curvature along the length of the
step.

Naively, I expect convergence to be cubic - the error comes from the
change in normal perpendicular to the current plane of curviture, so
it's got a derivative-of-a-derivative element to it. Furthermore, we
expect the the error to be proportional to that second
derivative/twistiness term.

While I could try to convert my intuition into some formal maths, I'd
much rather do some practical tests on my application and see how it
behaves.

## First practical results

*I've tweaked the algorithms as a result of these analyses. As such,
if you're trying to reproduce these numbers, use a version at
`1ad2f40` or before.*

My theory was *totally* wrong! I'm keeping in the text above as an
example of my hubris. :p

```
$ cargo run --bin convergence-test --release -- -o ratios -v step-dir | tee ratios.csv
```

Looking at [ratios.csv](./ratios.csv), generated by convergence-test,
we can see that, over the various paths, each time the step size is
doubled, the error is doubled (the first number on each row is off,
because it's basing the error prediction on the smallest step size
being fully correct).

Trying to rejig my intuition to match reality, I guess the simplest
version of "tracing a path through a curve space embedded in a higher
dimension" is a polygonal approximation to a circle, and there, the
errors in the approximation are linear with step size. Ho hum.

From here, I want to understand the error behaviour better - the error
in position vs. the error in direction of the final ray, how the error
accumulates over the ray, and how that relates to curvature along the
ray, etc. 

### Further path-level results

I added support for finding not just the error in the direction as
calculated by the final step, but also the error as calculated by
adjusting that for the normal at the final point, and for finding the
error in the final position:

```
$ cargo run --bin convergence-test --release -- -v step-dir | tee step-err.csv
$ cargo run --bin convergence-test --release -- -v deriv-dir | tee deriv-err.csv
$ cargo run --bin convergence-test --release -- -v point | tee point-err.csv
```

In all cases, the size of the error is roughly the same, and
convergence is linear. Very roughly, the error is 1/100th of the step
size.

If we're to use a uniform step size, and the environment map is
2048x2048, mapped onto the range -1..1, we want a precision of about
0.001, which suggests using a step size of 0.1. I've been using a
RAY_STEP value of 0.01, and it sounds like I could safely bump it up
quite a bit.

### Reduced step size and Newton-Raphson iterations

On the other hand, I'm a bit suspicious, because 0.1 is a rather large
step size when the wormhole radius is 0.1. I wonder if the automatic
step-size-reduction code is helping out?

(The step size is automatically reduced when the Newton-Raphson solver
doesn't converge, as identified by taking more than 10 steps.)

I hacked up a branch of the code to report on when paths required
step-shortening to ensure convergence. Up to step size 0.032, no step
halving was performed. For a step size of 0.064 (the largest the tests
do), 280 paths out of 4096 required step shortening, 168 for a single
step, and 112 for two steps. So, it's applied sparingly and only at
the largest step size, and doesn't seem to be a major factor in
convergence,

Diving in further, and looking at how quickly the Newton-Raphson
solver converges, at step size 0.016 everything converges in 2 or
fewer steps. At step sizes 0.032 and 0.064:

 * many rays take 3 iterations for some steps (specifically, those
   near or that go through the wormhole),
 * a tiny fraction of steps take 4 or 5 iterations, and
 * a similar number of steps do not converge, requiring step size
   reduction.

No cases converge in 7-10 steps.

(The cleaned-up output of this output, produced in the branch
`step-stats` is in the file
[step-convergence.log](./step-convergence.log).)

Since the cases that require more iterations are ones that probably
introduce more error, it looks like there's a case for step-splitting
if convergence doesn't happen after e.g. 2 steps. This is going to be
worth investigating after a quick investigation as to how step size
affects visual quality.

### Step size and visual quality

I then did some empirical tests, comparing the output of various step
sizes from 0.001 to 0.08 when rendering a 1024x768 image, 90 degree
field of view:

```
for SIZE in 0.001 0.002 0.004 0.008 0.01 0.02 0.04 0.08
    do time cargo run --release --bin tray-racer-cli -- -s $SIZE -o step-${SIZE}.png
done
```

The fine detail of what's visible through the wormhole seems to be the
most visibly sensitive part, and looks pretty consistent up to
0.01. With a step size beyond that, it begins to distort.

How do I square this with "the error is 1/100th of the step size", the
approximation of error I found earlier?

Going back to the original data, and looking at the full range of
data, including outliers, we see that at each step size most paths are
converged, but there are a few outliers with much bigger errors than
the other rays. As the step size increases, the number of outliers and
the size of the error compared to the median increases:

| Step size                | 0.02        | 0.04        | 0.08        | 0.16        | 0.32       | 0.64        |
| Max error / median error | 3.381478643 | 2.259884166 | 7.781941461 | 19.01173447 | 42.7861627 | 497.8668368 |
| % errors > 5x median     | 0.00%       | 0.00%       | 0.68%       | 20.02%      | 25.20%     | 28.61%      |

It now becomes obvious that the key is to make sure we set step sizes
to ensures that the tail of the *distribution* of errors isn't too
long and fat - that the rendering has to make the trickiest paths look
right, not just the common case. In turn, this suggests we need
adaptive step sizes, since we don't want to waste time on tiny steps
on the common case where it's not needed, in order to ensure we get a
small enough step size on the tricky cases.

### Limiting Newton-Raphson iterations

If the vast majority of rays converge in 0-2 Newton-Raphson
iterations, maybe those that take more iterations to complete are the
ones contributing most to error? I tried limiting the iterations on
path steps to 0-2 and visually inspected the results for different
step sizes. Performance was pretty much the same as the base case. As
before, some visual distortion was seen at step size 0.02 (0.01 looked
ok), but unlike before, for larger step sizes the image quality didn't
degrade further. It looks like this heuristic *does* detect and
correct for steps that accumulate too much error, but at a scale large
enough to cause visual degradation.

Interestingly, while the visual quality at large step sizes is better,
the stats do not improve:

| Step size                | 0.002       | 0.004       | 0.008       | 0.016       | 0.032       | 0.064       |
| Max error / median error | 3.381478643 | 2.259884166 | 7.781941461 | 19.01173447 | 68.43652523 | 338.2307236 |
| % errors > 5x median     | 0.00%       | 0.00%       | 0.68%       | 20.02%      | 25.98%      | 27.54%      |

### Step-level analysis

**TODO: Somehow we need to adapt the step size with error
contribution, which in turn is probably related to curvature. How best
to achieve this?**

Do we really need this if Newton-Raphson-convergence-based step size
changing seems to do the job?
