# Convergence testing

Given that rendering a scene involves firing a lot of rays, we want to
make each ray cheap. Each ray involves stepping along the path, so
minimising the number of steps seems a good idea.

Tracing along the path is effectively a numerical differential
equation solver, and the convergence on the true solution is going to
depend on the step size. This trade-off between accuracy and step size
is what this project intends to explore.

## Vague theory

There's a complicating factor in that if the path is overall
divergent, a small error at the start can accumulate over the course
of the path. That is, the way errors accumulate along the path depend
on positive and negative curvature. For simplicity's sake, I'm going
to ignore this, but I suspect that this is also a good reason why I
should look at the empirical convergence behaviour (see below).

Assuming (incorrectly) that the final error is simply an accumulation
of step-wise error along the path, we can look into the local
convergence properties. If this were simply a linear approximation to
a curve, we would expect the convergence to be quadratic, as error
accumulates from the second-order term being approximated
linearly. However, we don't do a linear approximation: the whole point
of the algorithm is to take a linear step and then solve for a point
in the surface to take account of the local curvature. The error in
the step comes from the change in curvature along the length of the
step.

Naively, I expect convergence to be cubic - the error comes from the
change in normal perpendicular to the current plane of curviture, so
it's got a derivative-of-a-derivative element to it. Furthermore, we
expect the the error to be proportional to that second
derivative/twistiness term.

While I could try to convert my intuition into some formal maths, I'd
much rather do some practical tests on my application and see how it
behaves.

## First practical results

My theory was *totally* wrong! I'm keeping in the text above as an
example of my hubris. :p

```
$ cargo run --bin convergence-test --release -- -o ratios -v step-dir | tee ratios.csv
```

Looking at [ratios.csv](./ratios.csv), generated by convergence-test,
we can see that, over the various paths, each time the step size is
doubled, the error is doubled (the first number on each row is off,
because it's basing the error prediction on the smallest step size
being fully correct).

Trying to rejig my intuition to match reality, I guess the simplest
version of "tracing a path through a curve space embedded in a higher
dimension" is a polygonal approximation to a circle, and there, the
errors in the approximation are linear with step size. Ho hum.

From here, I want to understand the error behaviour better - the error
in position vs. the error in direction of the final ray, how the error
accumulates over the ray, and how that relates to curvature along the
ray, etc. 

### Further path-level results

I added support for finding not just the error in the direction as
calculated by the final step, but also the error as calculated by
adjusting that for the normal at the final point, and for finding the
error in the final position:

```
$ cargo run --bin convergence-test --release -- -v step-dir | tee step-err.csv
$ cargo run --bin convergence-test --release -- -v deriv-dir | tee deriv-err.csv
$ cargo run --bin convergence-test --release -- -v point | tee point-err.csv
```

In all cases, the size of the error is roughly the same, and
convergence is linear. Very roughly, the error is 1/100th of the step
size.

If we're to use a uniform step size, and the environment map is
2048x2048, mapped onto the range -1..1, we want a precision of about
0.001, which suggests using a step size of 0.1. I've been using a
RAY_STEP value of 0.01, and it sounds like I could safely bump it up
quite a bit.

### Reduced step size and Newton-Raphson iterations

On the other hand, I'm a bit suspicious, because 0.1 is a rather large
step size when the wormhole radius is 0.1. I wonder if the automatic
step-size-reduction code is helping out?

(The step size is automatically reduced when the Newton-Raphson solver
doesn't converge, as identified by taking more than 10 steps.)

I hacked up a branch of the code to report on when paths required
step-shortening to ensure convergence. Up to step size 0.032, no step
halving was performed. For a step size of 0.064 (the largest the tests
do), 280 paths out of 4096 required step shortening, 168 for a single
step, and 112 for two steps. So, it's applied sparingly and only at
the largest step size, and doesn't seem to be a major factor in
convergence,

Diving in further, and looking at how quickly the Newton-Raphson
solver converges, at step size 0.016 everything converges in 2 or
fewer steps. At step sizes 0.032 and 0.064:

 * many rays take 3 iterations for some steps (specifically, those
   near or that go through the wormhole),
 * a tiny fraction of steps take 4 or 5 iterations, and
 * a similar number of steps do not converge, requiring step size
   reduction.

No cases converge in 7-10 steps.

(The cleaned-up output of this output, produced in the branch
`step-stats` is in the file
[step-convergence.log](./step-convergence.log).)

Since the cases that require more iterations are ones that probably
introduce more error, it looks like there's a case for step-splitting
if convergence doesn't happen after e.g. 2 steps. This is going to be
worth investigating after a quick investigation as to how step size
affects visual quality.

### Step size and visual quality

**TODO: Experiment with step size empirically!**

### Limiting Newton-Raphson iterations

**TODO**

### Step-level analysis

**TODO: In order to understand how error accumulates better, look at
how the error accumulates on a per-step basis, and compare with
curvature metrics. Is varying step size worth it?**

Do we really need this if Newton-Raphson-convergence-based step size
changing seems to do the job?
